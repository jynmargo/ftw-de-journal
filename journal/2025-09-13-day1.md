# Journal ‚Äî September 13, 2025 ‚Äî Week 1	Foundations & First Pipeline

## 1) What I learned
### üîß Tools


[![ClickHouse](https://img.shields.io/badge/ClickHouse-FFCC01?style=for-the-badge&logo=ClickHouse&logoColor=000)](https://clickhouse.com/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![DBeaver](https://img.shields.io/badge/DBeaver-372923?style=for-the-badge&logo=dbeaver&logoColor=white)](https://dbeaver.io/)
[![dltHub](https://img.shields.io/badge/dltHub-FF6F00?style=for-the-badge&logo=databricks&logoColor=white)](https://dlthub.com/) <!-- dltHub doesn‚Äôt have an official shields.io logo, so Databricks is used as a placeholder -->
[![Metabase](https://img.shields.io/badge/Metabase-509EE3?style=for-the-badge&logo=metabase&logoColor=white)](https://www.metabase.com/)
[Add DBT]

![Spongebob Meme](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTqQT0XUZ9S-eML_99jUjnbBOOpLihlTBpGtQ&s)

![Hierarchy of needs](https://towardsdatascience.com/wp-content/uploads/2021/06/0yMbTTTVP7IUe-doG.jpg)

![Data Governance](https://technologytransfer.it/wp-content/uploads/Foto-1.png)

![Sir Myk: Noong Unang Panahon...](https://tahananbooks.com/cdn/shop/products/NOONG-UNANG-PANAHON-01_1280x.png?v=1600418611)

![Medallion Architecture](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)


### ‚öôÔ∏è Main Project Process
- Creating Clickhouse Cloud
![ClickHouse Create Cloud](https://i.imgur.com/PNVqEAt.png)

![DBeaver](https://i.imgur.com/eFeD2SD.png)

![Metabase](https://i.imgur.com/3dXwQdJ.png)
![Metabase](https://i.imgur.com/W763QdY.png)
## SANDBOX
![Sandbox Database](https://cdnb.artstation.com/p/assets/images/images/026/487/301/original/yun-yun-hakusho-doof.gif?1588900713)

- Safe space to experiment, exploratory tool, data visualizatio-centric
- In this we created 2 databases


### 

#### General Data Engineering Concepts Learnings 
- **Cloud provider (AWS)** ‚Üí ClickHouse needs a server to run on. I chose AWS (Amazon Web Services).
- **Choosing Cloud Provider Region** ‚Üí This decides where physically your database will be hosted. Closer regions = lower latency.
- ClickHouse Cloud is freemium. They usually offer a free trial (something like $300 in credits, or a certain number of hours of usage) so you can test it without paying immediately.
- **Replicas** = how many ‚Äúservers‚Äù run your database.
- **GiB RAM** = memory for storing/handling queries.
- **vCPU** = how many ‚Äúbrains‚Äù process your data.
- **Scaling** - Think of employee üßëüèª‚Äçüíª == machine üíª
    - **Horizontal Scaling** = üßëüèª‚Äçüíª + üßëüèª‚Äçüíªüßëüèª‚Äçüíªüßëüèª‚Äçüíª
    - **Vertical Scaling** = üßëüèª‚Äçüíª + üß†üìöüìì

- **Data Imputation** - replacing miss, incorrect, or inconsistent data with estimated, sensible values from the available data.
- **Coalesce** - how to handle null values
- **Twitter API** - 1% data is still very big
- **Engineering** - thinking about trade-offs
### Lesson proper
- Data Audit - what if data loaded is transformed, but raw data isn't stored? ETL process before no longer stores the raw data.


## 2) New vocabulary (define in your own words)
- **Edge Cases** - unusual, rare, or extreme conditions that push a system to its operational limits, often revealing vulnerabilities and causing unexpected failures or incorrect results
- **ETL, ELT**

![OLAF](https://media.tenor.com/i8etyEsz7LQAAAAM/olaf-elsa.gif)
- **OLAP** - Online Analytical Processing (OLAP ‚òÉ) 
    - House all information for querying. Excel has limits - this information can get crazy. Sir Myk has seen rows billions, 10,000 columns. For performance and reliability -> Database
    - In the past 5 years, ano ang trend ng sales?
- **OLTP** - Online LTP - Transaction, Sequential, Events, Transactions
    - Representative History of the Truth 
    - Production (Untouchable). Nangyari ba transaction o hindi? 
    - Corporate - Day to Day. (Ilan today nabili)
    - Row by row recording
- **Blob** - Binary Large object
- **NUP**: Small data for older systems
- **CTAS** Create table


## 3) Data Engineering mindset applied (what principles did I use?)
- Fix pipelines, optimize, master new tools, bridge teams, scalable systems 
- Big Data - Volume, Variety, Velocity, Veracity, Variability, Value
(NUP - 1GB data was big)
- Cleaning and Standardization happens in stages
- We're not just moving data from places. We make sure data is TRUSTED.
- Architectural Practices depend on business requirements (no right or wrong, only best practices)
- Structure allows auditability
- DE is full of tradeoffs. Fast/clean. You need to be able to work around the tradeoffs 

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- I know whow 

## 5) Open questions (things I still don‚Äôt get)
- **Imputation** - How to use imputation on missing values?

## 6) Next actions (small, doable steps)
- [ ] Practice more advanced SQL querying particularly joining. Still in trouble with it.
- [ ] Study about Schema Flexibility
- [ ] 3rd Saturday Streaming
- [ ] Continue studying the architecture after FTW
- [ ] Documentation (mapping)
- [ ] Metabase Data Visualziation Metabase hosted on cloud
## 7) Artifacts & links (code, queries, dashboards)
### üìö Data Engineering
- [Awesome Data Engineering resources](https://github.com/igorbarinov/awesome-data-engineering)  
- [Dataset Repository](https://archive.ics.uci.edu/dataset/9/auto+mpg)  

### ‚úçÔ∏è Markdown Enhance / Maker
- [Markdown Preview](https://markdownlivepreview.com/)  
- [Markdown Badges for tech tools](https://alexandresanlim.github.io/Badges4-README.md-Profile/#/?id=%e2%9a%a1-database-%f0%9f%94%9dmenu)  
- [Shields.io ‚Äì More MD Custom Badges](https://shields.io/)  
- [Poe.com ‚Äì AI Aggregation](Poe.com)  
- [Joining Tables](https://www.youtube.com/watch?v=zGSv0VaOtR0)
---

### Mini reflection (3‚Äì5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production?

I have come to slowly appreciate the whole proces of Data Engineering. I liked how we were able to turn this raw data into powerful visualizations that help tell stories better.
